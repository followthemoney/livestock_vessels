{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openmeteo import get_meteo\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = Path.cwd().parent.joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vessel tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tracks from Global Fishing Watch\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in PATH_DATA.joinpath('gfw_tracks').glob('*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    filename = file.stem\n",
    "    df['vessel'] = filename\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')\n",
    "df['date_normalised'] = df['date'].dt.normalize().astype('str')\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df.sort_values(by='date', inplace=True)\n",
    "df = df[df.speed > 0].copy()\n",
    "df = df[df['date'] >= '2023-11-01'].copy()\n",
    "#df = df[(df.hour== 14) | (df.hour == 2)].copy()\n",
    "df = df.drop_duplicates(subset=['vessel', 'date_normalised', 'hour'], keep='first')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select vessels\n",
    "\n",
    "vessels = ['sarahm', 'ganadoexpress']\n",
    "batch = df[df.vessel.isin(vessels)].copy()\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WWO_API_KEY = os.environ.get('WWO_API_KEY')\n",
    "BASE_URL = 'https://api.worldweatheronline.com/premium/v1/past-marine.ashx'\n",
    "\n",
    "results = []\n",
    "failed = []\n",
    "\n",
    "for i, row in batch.iterrows():\n",
    "    start = row.date_normalised\n",
    "    lat = row.lat\n",
    "    lon = row.lon\n",
    "    \n",
    "    url = f'{BASE_URL}?key={WWO_API_KEY}&q={lat},{lon}&format=json&date={start}'\n",
    "    result = requests.get(url)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Reached index {i}')\n",
    "\n",
    "    if result.status_code == 200 or result.status_code == 201:\n",
    "        result = result.json()\n",
    "        \n",
    "        if result.get('data').get('error') is not None:\n",
    "            with open(PATH_DATA.joinpath('meteo', 'meteo_select_results_failed.json'), 'a') as file:\n",
    "                result = {\"vessel\": row.vessel,\n",
    "                          \"index\": i,\n",
    "                          \"response\": result}\n",
    "                file.write(f'{result}\\n')\n",
    "            failed.append(i)\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            result.update({'lon': row.lon,\n",
    "                           'lat': row.lat,\n",
    "                           'vessel': row.vessel,\n",
    "                           'timestamp': row.timestamp,\n",
    "                           })\n",
    "        \n",
    "            with open(PATH_DATA.joinpath('meteo', 'meteo_select_results.json'), 'a') as file:\n",
    "                file.write(f'{result}\\n')\n",
    "            \n",
    "            results.append(result)\n",
    "    else:\n",
    "        failed.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data\n",
    "\n",
    "daily = []\n",
    "hourly = []\n",
    "for r in results:\n",
    "    if r.get('data').get('weather') is not None:\n",
    "        data = r.get('data').get('weather')[0]\n",
    "        query = {'lon': r.get('lon'),\n",
    "                'lat': r.get('lat'),\n",
    "                'vessel': r.get('vessel'),\n",
    "                'timestamp': r.get('timestamp')\n",
    "                }\n",
    "        d = {}\n",
    "        for k, v in data.items():\n",
    "            \n",
    "            if k == 'astronomy':\n",
    "                d.update(v[0].items())\n",
    "            if k != 'hourly' and k !='astronomy':\n",
    "                d.update({k: v})\n",
    "        d.update(query)\n",
    "        daily.append(d)\n",
    "        \n",
    "\n",
    "        data = r.get('data').get('weather')[0].get('hourly')\n",
    "        d = {}\n",
    "\n",
    "        for d in data:\n",
    "            for k, v in d.items():\n",
    "                d.update({k: v})\n",
    "        d.update(query)\n",
    "        hourly.append(d)\n",
    "\n",
    "df_daily = pd.DataFrame(daily)\n",
    "df_hourly = pd.DataFrame(hourly)\n",
    "\n",
    "# Write to file\n",
    "df_daily.to_csv(PATH_DATA.joinpath('meteo', 'meteo_results_daily.csv'), index=False, mode='a', header=None)\n",
    "df_hourly.to_csv(PATH_DATA.joinpath('meteo', 'meteo_results_hourly.csv'), index=False, mode='a', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.to_csv(PATH_DATA.joinpath('meteo', 'selected_vessels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weather Data - Historical Weather API \n",
    "\n",
    "[This historical weather API](https://open-meteo.com/en/docs/historical-weather-api) is available for quering weather data along the route. The historical data isn't specialised in maritime environments though, so another service might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i, row in gdf.iterrows():\n",
    "    start = row['date_normalised']\n",
    "    end = row['date_normalised']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    df = get_meteo(lon=lon, lat=lat, start_date=start, end_date=end)\n",
    "    df['query_lat'] = lat\n",
    "    df['query_lon'] = lon\n",
    "    df['query_start_date'] = start\n",
    "    df['query_end_date'] = end\n",
    "    df['query_hour'] = row.hour\n",
    "    dfs.append(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
